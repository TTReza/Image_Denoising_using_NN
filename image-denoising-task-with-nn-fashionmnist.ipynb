{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9243,"sourceType":"datasetVersion","datasetId":2243}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-26T18:20:40.486568Z","iopub.execute_input":"2024-03-26T18:20:40.486938Z","iopub.status.idle":"2024-03-26T18:20:41.660125Z","shell.execute_reply.started":"2024-03-26T18:20:40.486908Z","shell.execute_reply":"2024-03-26T18:20:41.658777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zipfile\nfrom urllib import request\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport random\nimport tensorflow as tf\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:22:19.898604Z","iopub.execute_input":"2024-03-26T18:22:19.899004Z","iopub.status.idle":"2024-03-26T18:22:34.781997Z","shell.execute_reply.started":"2024-03-26T18:22:19.898973Z","shell.execute_reply":"2024-03-26T18:22:34.780751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_database = np.loadtxt('/kaggle/input/fashionmnist/fashion-mnist_train.csv', delimiter= ',', skiprows = 1)[:, 1:]\nprint(img_database.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:24:03.743204Z","iopub.execute_input":"2024-03-26T18:24:03.744778Z","iopub.status.idle":"2024-03-26T18:24:09.420074Z","shell.execute_reply.started":"2024-03-26T18:24:03.744713Z","shell.execute_reply":"2024-03-26T18:24:09.419049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 60000 images and the dimension is 784,1. It basically means that the images has been flattened out to an one dimensional array of 784,1 from the actual dimension of 28X28","metadata":{}},{"cell_type":"code","source":"total_num_images = (img_database.shape[0])","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:24:31.671250Z","iopub.execute_input":"2024-03-26T18:24:31.672254Z","iopub.status.idle":"2024-03-26T18:24:31.676945Z","shell.execute_reply.started":"2024-03-26T18:24:31.672140Z","shell.execute_reply":"2024-03-26T18:24:31.675883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The Neural Network Archi**","metadata":{}},{"cell_type":"code","source":"import tensorflow.compat.v1 as tf\n\ntf.disable_v2_behavior()","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:42:38.254921Z","iopub.execute_input":"2024-03-26T18:42:38.255312Z","iopub.status.idle":"2024-03-26T18:42:38.266448Z","shell.execute_reply.started":"2024-03-26T18:42:38.255280Z","shell.execute_reply":"2024-03-26T18:42:38.265227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"tensorflow.placeholder was not working so found above solution in StackOverFlow. :)|","metadata":{}},{"cell_type":"code","source":"#structuring the input and output layers\nn_input = 784\nhidden_layer1 = 256\nhidden_layer2 = 32\nhidden_layer3 = 32\nhidden_layer4 = 256\noutput_layer = 784 #this is mainly encoding and decoding\n\nlearning_rate = 0.1\nepochs = 150\nbatch_size = 100\n\n\n#placeholders\nX = tf.placeholder(tf.float32, [None, n_input])\nY = tf.placeholder(tf.float32, [None, output_layer])\n\n#weight and biases\nWeight_NN = {\"W1\": tf.Variable(tf.random_normal([n_input, hidden_layer1])),\n             \"W2\": tf.Variable(tf.random_normal([hidden_layer1, hidden_layer2])),\n            \"W3\": tf.Variable(tf.random_normal([hidden_layer2, hidden_layer3])),\n            \"W4\": tf.Variable(tf.random_normal([hidden_layer3, hidden_layer4])),\n            \"W5\": tf.Variable(tf.random_normal([hidden_layer4, output_layer]))\n            }\nBias_NN = { \"B1\": tf.Variable(tf.random_normal([hidden_layer1])),\n           \"B2\": tf.Variable(tf.random_normal([hidden_layer2])),\n          \"B3\": tf.Variable(tf.random_normal([hidden_layer3])),\n          \"B4\": tf.Variable(tf.random_normal([hidden_layer4])),\n          \"B5\": tf.Variable(tf.random_normal([output_layer]))}","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:42:55.469282Z","iopub.execute_input":"2024-03-26T18:42:55.469727Z","iopub.status.idle":"2024-03-26T18:42:55.545501Z","shell.execute_reply.started":"2024-03-26T18:42:55.469689Z","shell.execute_reply":"2024-03-26T18:42:55.544144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Point to be noted*: The variable constructor for weights and biases are used because they will be changing.","metadata":{}},{"cell_type":"markdown","source":"* **Placeholders:** A placeholder is simply a variable that we will assign data to at a later date. It allows us to create our operations and build our computation graph, without needing the data.\n* **The Variable()** constructor requires an initial value for the variable, which can be a Tensor of any type and shape. This initial value defines the type and shape of the variable. After construction, the type and shape of the variable are fixed.\n","metadata":{}},{"cell_type":"code","source":"#forward pass for the neural network\nZ1 = tf.add(tf.matmul(X, Weight_NN[\"W1\"]), Bias_NN[\"B1\"])\nZ1_out = tf.nn.sigmoid(Z1) #non linearity applied\n\nZ2 = tf.add(tf.matmul(Z1_out, Weight_NN[\"W2\"]), Bias_NN[\"B2\"])\nZ2_out = tf.nn.sigmoid(Z2)\n\nZ3 = tf.add(tf.matmul(Z2_out, Weight_NN[\"W3\"]), Bias_NN[\"B3\"])\nZ3_out = tf.nn.sigmoid(Z3)\n\nZ4 = tf.add(tf.matmul(Z3_out, Weight_NN[\"W4\"]), Bias_NN[\"B4\"])\nZ4_out = tf.nn.sigmoid(Z4)\n\nZ5 = tf.add(tf.matmul(Z4_out, Weight_NN[\"W5\"]), Bias_NN[\"B5\"])\nNN_output = (Z5)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:50:28.879299Z","iopub.execute_input":"2024-03-26T18:50:28.879752Z","iopub.status.idle":"2024-03-26T18:50:28.899798Z","shell.execute_reply.started":"2024-03-26T18:50:28.879719Z","shell.execute_reply":"2024-03-26T18:50:28.898572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the same NN using tensorflow api\n#Z1 = tf.layers.dense(X, hidden_layer1, activation = tf.nn.sigmoid)\n#Z2 = tf.layers.dense(Z1, hidden_layer2, activation = tf.nn.sigmoid)\n#Z3 = tf.layers.dense(Z2, hidden_layer3, activation = tf.nn.sigmoid)\n#Z4 = tf.layers.dense(Z3, hidden_layer4, activation = tf.nn.sigmoid)\n#NN_output = tf.layers.dense(Z4, output_layer)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T19:05:16.266053Z","iopub.execute_input":"2024-03-26T19:05:16.266621Z","iopub.status.idle":"2024-03-26T19:05:16.273031Z","shell.execute_reply.started":"2024-03-26T19:05:16.266576Z","shell.execute_reply":"2024-03-26T19:05:16.271541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dataset Processing**","metadata":{}},{"cell_type":"code","source":"np.random.shuffle(img_database) #shuffling\nX_train = img_database\nX_train = X_train #normalize\nX_train_noisy = X_train + 10*np.random.normal(0, 1, size = X_train.shape) #creating noisy dataset\n\n#original img\nplt.imshow(X_train[0].reshape(28,28), cmap = 'gray')\nplt.show()\n\n#noisy img\nplt.imshow(X_train_noisy[0].reshape(28,28), cmap = 'gray')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:54:19.412240Z","iopub.execute_input":"2024-03-26T18:54:19.413518Z","iopub.status.idle":"2024-03-26T18:54:21.829560Z","shell.execute_reply.started":"2024-03-26T18:54:19.413479Z","shell.execute_reply":"2024-03-26T18:54:21.828492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loss function\ncomputed_loss = tf.reduce_mean(tf.square(NN_output-Y))\noptimizer = tf.train.AdagradOptimizer(learning_rate).minimize(computed_loss)\ninit = tf.global_variables_initializer() #these are the wieghts and biases","metadata":{"execution":{"iopub.status.busy":"2024-03-26T19:00:47.912698Z","iopub.execute_input":"2024-03-26T19:00:47.913111Z","iopub.status.idle":"2024-03-26T19:00:48.051543Z","shell.execute_reply.started":"2024-03-26T19:00:47.913079Z","shell.execute_reply":"2024-03-26T19:00:48.050307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*This optimizer in neural network works as follows:*\n\n* It begins with a few coefficients, observes their cost, and finds a cost value lower than what it is currently.\n* It proceeds towards the lower weight and updates the coefficients’ value.\n* The process iterates until the local minimum is attained. A local minimum is a point outside which it can’t proceed.","metadata":{}},{"cell_type":"markdown","source":"**Execution of the Neural Network**","metadata":{}},{"cell_type":"code","source":"ses = tf.Session()\nses.run(init)\nfor epoch in range(epochs):\n    for i in range(int(total_num_images/batch_size)):\n        X_epoch = X_train[i*batch_size : (i+1)*batch_size]\n        X_noise_epoch = X_train_noisy[i*batch_size : (i+1)*batch_size ]\n        _, loss = ses.run([optimizer, computed_loss], feed_dict={X: X_noise_epoch, Y: X_epoch})\n    print('Epoch', epoch, '/', epochs, 'loss: ', loss)\n\n#choosing an img\nX_actual = X_train[20:30]\nnoisy_image = X_train_noisy[20:30]\n\ndenoised = ses.run(NN_output, feed_dict = {X:noisy_image})\n\n#printing\nfig, axes = plt.subplots(nrows = 3, ncols = 10, sharex = True, sharey= True, figsize=(20,4))\nfor images, row in zip([X_actual, noisy_image, denoised], axes):\n    for img, ax in zip(images, row):\n        ax.imshow(img.reshape((28,28)), cmap = 'Greys_r')\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n        \n\nfig.tight_layout(pad = 0.1)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T19:33:01.985822Z","iopub.execute_input":"2024-03-26T19:33:01.986217Z","iopub.status.idle":"2024-03-26T19:41:07.803557Z","shell.execute_reply.started":"2024-03-26T19:33:01.986188Z","shell.execute_reply":"2024-03-26T19:41:07.802211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The X is mapped to X_noise_epoch because here what we are doing at the beginning of the neural network is that, we are making the original images noisy first. Then the output Y is mapped to the image of X_epochs which are the denoised version.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This is just an implementation using neural networks. Using CNN can further improve the denoising**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}